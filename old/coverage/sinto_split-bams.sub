#!/bin/bash
#SBATCH --job-name=sinto
#SBATCH -p standard
#SBATCH -A vswarup_lab
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --error=slurm-%J.err
#SBATCH --mem 64G
#SBATCH --array=0-10
#SBATCH --time=72:00:00

# activate conda env with sinto + deeptools installed
source ~/.bashrc
conda activate scvi-env
module load samtools

# directory with barcodes files:
barcodes_dir="/dfs3b/swaruplab/smorabit/collab/woodlab/cocaine_mouse_2021/Nurr2c_vs_GFP/trackhubs/data/barcodes/"

# cellranger directory:
cellranger_dir="/dfs3b/swaruplab/smorabit/collab/woodlab/cocaine_mouse_2021/data/cellranger/"

# directory where we will output bam files:
output_dir="/dfs3b/swaruplab/smorabit/collab/woodlab/cocaine_mouse_2021/Nurr2c_vs_GFP/trackhubs/data/bams/"

# mkdir -p $output_dir/celltypes
# mkdir -p $output_dir/clusters

# select barcodes file based on the task array:
sample_names=($(ls $barcodes_dir | cut -d '_' -f 1-3 | uniq))
samples=($(ls $barcodes_dir | cut -d '_' -f 1-4 | uniq))

# get current sample based on the SLURM job ID
let index="$SLURM_ARRAY_TASK_ID"
sample=${samples[$index]}
sample_name=${sample_names[$index]}
echo $sample
echo $sample_name

# make output folders for this sample:
mkdir $output_dir/$sample

################################################################################
# sinto processing
################################################################################

# run sinto to split .bam file by barcodes
cd $output_dir/$sample

sinto filterbarcodes \
  -b $cellranger_dir/$sample_name/"outs/possorted_genome_bam.bam" \
  -c $barcodes_dir/$sample"_barcodes.tsv" \
  -p  32 # number of processors

# index all of the bam files in this folder:
for f in *; do
  samtools index -@ 32 $f
done
